{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1440253c-9988-4ca5-8778-0cd3611715c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 12.0233 | Test Loss: 7.6606\n",
      "Epoch 002 | Train Loss: 8.1615 | Test Loss: 6.7657\n",
      "Epoch 003 | Train Loss: 7.4450 | Test Loss: 6.2259\n",
      "Epoch 004 | Train Loss: 6.8427 | Test Loss: 5.4380\n",
      "Epoch 005 | Train Loss: 6.3984 | Test Loss: 5.1703\n",
      "Epoch 006 | Train Loss: 5.9572 | Test Loss: 4.6681\n",
      "Epoch 007 | Train Loss: 5.6964 | Test Loss: 5.0643\n",
      "Epoch 008 | Train Loss: 5.4826 | Test Loss: 3.8405\n",
      "Epoch 009 | Train Loss: 5.2652 | Test Loss: 3.5757\n",
      "Epoch 010 | Train Loss: 5.0809 | Test Loss: 3.3433\n",
      "Epoch 011 | Train Loss: 4.9621 | Test Loss: 3.3695\n",
      "Epoch 012 | Train Loss: 4.7723 | Test Loss: 3.2634\n",
      "Epoch 013 | Train Loss: 4.6646 | Test Loss: 2.8319\n",
      "Epoch 014 | Train Loss: 4.5641 | Test Loss: 2.9317\n",
      "Early stopping triggered\n",
      "\n",
      "Final RMSE: 1.6828 months\n"
     ]
    }
   ],
   "source": [
    "# I have trained this data only using the file called training.xlx \n",
    "# I am not sure what how to use the truncated data or novice data aloing with this data to get a combined model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------\n",
    "# 1. Data Loading & Prep\n",
    "# ----------------------\n",
    "def load_and_preprocess():\n",
    "    \"\"\"\n",
    "    This function handles loading the data file and initial preprocessing.\n",
    "    It removes invalid data points and selects relevant features.\n",
    "    \"\"\"\n",
    "    # Load data from CSV file with low_memory=False to prevent mixed-type inference\n",
    "    # Here you should insert your own path to the CSV file.\n",
    "    df = pd.read_csv('/Users/gautam/Downloads/Training.csv', low_memory=False)\n",
    "    \n",
    "    # Define the target variable (what we want to predict - match duration)\n",
    "    # Here I was only trying to predict the match length. I don't remeber if the questiosn in the required us to predict anything else \n",
    "    target = 'Match Length'\n",
    "    \n",
    "    # Data cleaning: Remove rows where the target is missing or negative\n",
    "    # This is critical since we can't train on invalid target values\n",
    "    # I don't think there are any negative values, but jsut to be sure \n",
    "    df = df[df[target].notna() & (df[target] > 0)]\n",
    "    \n",
    "    # Feature selection: Drop columns that wouldn't be useful for prediction:\n",
    "    # - Unique identifiers (Match ID)\n",
    "    # - Date fields that might leak future information (Completion Date)\n",
    "    # - Text fields that are hard to process (Notes, Closure Details)\n",
    "    # - Redundant or irrelevant features\n",
    "    features = df.drop(columns=[\n",
    "        target, 'Match ID 18Char', 'Completion Date', \n",
    "        'Match Support Contact Notes', 'Closure Details',\n",
    "        'Rationale for Match', 'Big Contact: Preferred Communication Type',\n",
    "        'Big Contact: Former Big/Little', 'Big Contact: Interest Finder - Sports',\n",
    "        'Big Contact: Interest Finder - Places To Go', 'Big Contact: Interest Finder - Hobbies',\n",
    "        'Big Contact: Interest Finder - Entertainment', 'Big Contact: Created Date'\n",
    "    ])\n",
    "    \n",
    "    return df, features, target\n",
    "\n",
    "# Execute the data loading function\n",
    "df, features, target = load_and_preprocess()\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Feature Preprocessing\n",
    "# ----------------------------\n",
    "def get_preprocessor(features):\n",
    "    \"\"\"\n",
    "    Creates a preprocessing pipeline that handles both numerical and categorical features.\n",
    "    This ensures consistent transformation of features for both training and inference.\n",
    "    \"\"\"\n",
    "    # Automatically identify categorical and numerical columns based on their data types\n",
    "    cat_cols = features.select_dtypes(include=['object', 'category']).columns\n",
    "    num_cols = features.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Remove constant numerical features that don't provide any information\n",
    "    # Features with zero variance (same value in all rows) are useless for prediction\n",
    "    constant_filter = VarianceThreshold(threshold=0)\n",
    "    constant_filter.fit(features[num_cols])\n",
    "    non_constant_num = num_cols[constant_filter.get_support()]\n",
    "    \n",
    "    # Create a composite preprocessing pipeline using ColumnTransformer\n",
    "    # This applies different transformations to different column types\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Numerical features pipeline:\n",
    "            ('num', Pipeline([\n",
    "                # Replace missing values with median (robust to outliers)\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                # Standardize features to zero mean and unit variance\n",
    "                # This helps neural networks converge faster\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), non_constant_num),\n",
    "            \n",
    "            # Categorical features pipeline:\n",
    "            ('cat', Pipeline([\n",
    "                # Replace missing values with a constant string 'missing'\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                # One-hot encode categorical variables\n",
    "                # handle_unknown='ignore' prevents errors with new categories at inference time\n",
    "                # max_categories=20 limits one-hot encoding to top 20 categories to prevent dimension explosion\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore', max_categories=20))\n",
    "            ]), cat_cols)\n",
    "        ],\n",
    "        # Drop any remaining columns not explicitly handled\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = get_preprocessor(features)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Data Preparation\n",
    "# ----------------------------\n",
    "# Apply the preprocessing transformations to our features\n",
    "X = preprocessor.fit_transform(features)\n",
    "\n",
    "# Convert target to float32 (PyTorch's preferred float type) numpy array\n",
    "y = df[target].values.astype(np.float32)\n",
    "\n",
    "# Validation checks to ensure no NaN values remain\n",
    "# NaNs would cause training errors\n",
    "assert not np.isnan(X.data).any(), \"X contains NaN values after preprocessing\"\n",
    "assert not np.isnan(y).any(), \"y contains NaN values\"\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Helper function to convert data to PyTorch tensors\n",
    "# Handles both dense numpy arrays and sparse matrices from one-hot encoding\n",
    "def convert_to_tensor(sparse_matrix):\n",
    "    if isinstance(sparse_matrix, np.ndarray):\n",
    "        return torch.tensor(sparse_matrix, dtype=torch.float32)\n",
    "    # If sparse, convert to dense before creating tensor\n",
    "    return torch.tensor(sparse_matrix.todense(), dtype=torch.float32)\n",
    "\n",
    "# Convert our data to PyTorch tensors\n",
    "X_train_tensor = convert_to_tensor(X_train)\n",
    "X_test_tensor = convert_to_tensor(X_test)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Dataset & DataLoader\n",
    "# ----------------------------\n",
    "class MatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that wraps our feature and target tensors.\n",
    "    This allows efficient batching, shuffling, and parallel data loading.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a specific sample by index\"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MatchDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = MatchDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define batch size (number of samples processed together)\n",
    "# Larger batch sizes are more efficient but use more memory\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader objects that handle batching, shuffling, and more\n",
    "# Shuffling training data helps prevent the model from learning the order of samples\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Neural Network\n",
    "# ----------------------------\n",
    "class MatchPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network architecture for predicting match length.\n",
    "    A feed-forward network with two hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"Initialize the network with given input dimension\"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # First layer: input_size -> 128 neurons\n",
    "            nn.Linear(input_size, 128),\n",
    "            # ReLU activation introduces non-linearity\n",
    "            nn.ReLU(),\n",
    "            # Dropout randomly zeros 30% of neurons during training to prevent overfitting\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Second layer: 128 -> 64 neurons\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            # Less dropout in deeper layers\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Output layer: 64 -> 1 (single regression output)\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        # Initialize weights using optimal strategy\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using Kaiming initialization.\n",
    "        This helps with training deep networks by preventing vanishing/exploding gradients.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Kaiming/He initialization - ideal for ReLU activations\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                # Initialize biases to zero\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # squeeze() removes the unnecessary singleton dimension from output\n",
    "        return self.layers(x).squeeze()\n",
    "\n",
    "# Initialize the model with the correct input dimension\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = MatchPredictor(input_size)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Training Configuration\n",
    "# ----------------------------\n",
    "# HuberLoss is less sensitive to outliers than MSE\n",
    "# Better choice for real-world data that might have outliers\n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "# AdamW optimizer: Adam with weight decay (L2 regularization)\n",
    "# lr: learning rate - how quickly parameters are updated\n",
    "# weight_decay: penalizes large weights to prevent overfitting\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler: reduces learning rate when validation loss plateaus\n",
    "# This helps fine-tune the model when it's close to convergence\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Training Loop\n",
    "# ----------------------------\n",
    "def train_model(model, epochs=100):\n",
    "    \"\"\"\n",
    "    Train the model for a specified number of epochs.\n",
    "    Implements early stopping to prevent overfitting.\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')  # Initialize best loss to infinity\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Set model to training mode (enables dropout)\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Loop through batches of training data\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Zero the gradients before each batch\n",
    "            # This prevents gradient accumulation from previous batches\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            # Compute loss between predictions and true values\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping prevents exploding gradients\n",
    "            # Ensures stable training by limiting gradient magnitude\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update model parameters based on gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate batch loss\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation phase - no gradient updates\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "        with torch.no_grad():  # No gradient computation for efficiency\n",
    "            # Compute predictions on test set\n",
    "            test_preds = model(X_test_tensor)\n",
    "            # Calculate validation loss\n",
    "            test_loss = criterion(test_preds, y_test_tensor)\n",
    "            # Adjust learning rate based on validation loss\n",
    "            scheduler.step(test_loss)\n",
    "        \n",
    "        # Calculate average training loss for reporting\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1:03d} | Train Loss: {avg_train_loss:.4f} | Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if test_loss < best_loss:\n",
    "            # If we found a better model, save it\n",
    "            best_loss = test_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            # If validation loss hasn't improved and we're past warm-up\n",
    "            if epoch > 10:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break  # Stop training\n",
    "\n",
    "    # Load the best model found during training\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Training & Evaluation\n",
    "# ----------------------------\n",
    "# Train the model with early stopping\n",
    "model = train_model(model, epochs=100)\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():  # No gradients needed for inference\n",
    "    # Generate predictions\n",
    "    test_preds = model(X_test_tensor)\n",
    "    # Calculate final loss\n",
    "    final_loss = criterion(test_preds, y_test_tensor)\n",
    "    # Convert loss to RMSE (Root Mean Squared Error) for interpretability\n",
    "    # RMSE is in the same units as the target (months)\n",
    "    rmse = torch.sqrt(final_loss)\n",
    "    print(f'\\nFinal RMSE: {rmse.item():.4f} months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1b0c9b-2360-409c-bf0e-e75101cf4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Match Length: 35.20 months\n",
      "Based on 39345 valid records\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_match_length():\n",
    "    \n",
    "        # Load the data\n",
    "        df = pd.read_csv('/Users/gautam/Downloads/Training.csv', low_memory=False)\n",
    "            \n",
    "        # Clean the data and calculate average\n",
    "        avg_length = df['Match Length'].dropna().mean()\n",
    "        \n",
    "        # Print formatted result\n",
    "        print(f\"\\nAverage Match Length: {avg_length:.2f} months\")\n",
    "        print(f\"Based on {len(df['Match Length'].dropna())} valid records\")\n",
    "        \n",
    "\n",
    "# Run the calculation\n",
    "calculate_average_match_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235794e2-717d-4a07-81ed-0dbc4114f4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
