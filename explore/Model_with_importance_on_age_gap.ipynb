{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........... (step 1 of 2) Processing imputer, total=   0.0s\n",
      "[Pipeline] ............ (step 2 of 2) Processing scaler, total=   0.0s\n",
      "[ColumnTransformer] ........... (1 of 2) Processing num, total=   0.0s\n",
      "[Pipeline] ........... (step 1 of 2) Processing imputer, total=   0.0s\n",
      "[Pipeline] ........... (step 2 of 2) Processing encoder, total=   0.0s\n",
      "[ColumnTransformer] ........... (2 of 2) Processing cat, total=   0.0s\n",
      "Epoch 001 | Train Loss: 16.6439 | Test Loss: 15.2298\n",
      "Epoch 002 | Train Loss: 15.5236 | Test Loss: 14.8614\n",
      "Epoch 003 | Train Loss: 15.1532 | Test Loss: 14.3653\n",
      "Epoch 004 | Train Loss: 14.7662 | Test Loss: 13.7937\n",
      "Epoch 005 | Train Loss: 14.4474 | Test Loss: 13.4641\n",
      "Epoch 006 | Train Loss: 14.0511 | Test Loss: 13.2218\n",
      "Epoch 007 | Train Loss: 13.7014 | Test Loss: 12.9365\n",
      "Epoch 008 | Train Loss: 13.3682 | Test Loss: 12.5982\n",
      "Epoch 009 | Train Loss: 13.1385 | Test Loss: 12.0920\n",
      "Epoch 010 | Train Loss: 12.7686 | Test Loss: 11.5753\n",
      "Epoch 011 | Train Loss: 12.5859 | Test Loss: 11.3158\n",
      "Epoch 012 | Train Loss: 12.4795 | Test Loss: 11.0868\n",
      "Epoch 013 | Train Loss: 12.2080 | Test Loss: 10.8944\n",
      "Epoch 014 | Train Loss: 12.1564 | Test Loss: 10.7863\n",
      "Epoch 015 | Train Loss: 12.1083 | Test Loss: 10.6934\n",
      "Epoch 016 | Train Loss: 11.9191 | Test Loss: 10.2790\n",
      "Epoch 017 | Train Loss: 11.7504 | Test Loss: 10.5403\n",
      "Epoch 018 | Train Loss: 11.6363 | Test Loss: 10.4650\n",
      "Epoch 019 | Train Loss: 11.5454 | Test Loss: 10.5169\n",
      "Epoch 020 | Train Loss: 11.5144 | Test Loss: 10.4134\n",
      "Epoch 021 | Train Loss: 11.4551 | Test Loss: 10.8409\n",
      "Epoch 022 | Train Loss: 11.3137 | Test Loss: 10.1347\n",
      "Epoch 023 | Train Loss: 11.4335 | Test Loss: 10.1817\n",
      "Epoch 024 | Train Loss: 11.2420 | Test Loss: 9.8928\n",
      "Epoch 025 | Train Loss: 11.1578 | Test Loss: 9.8083\n",
      "Epoch 026 | Train Loss: 11.2127 | Test Loss: 9.7071\n",
      "Epoch 027 | Train Loss: 11.1046 | Test Loss: 9.7240\n",
      "Epoch 028 | Train Loss: 11.0889 | Test Loss: 9.6342\n",
      "Epoch 029 | Train Loss: 11.0607 | Test Loss: 10.0483\n",
      "Epoch 030 | Train Loss: 11.0589 | Test Loss: 10.1788\n",
      "Epoch 031 | Train Loss: 11.0039 | Test Loss: 9.8995\n",
      "Epoch 032 | Train Loss: 10.9913 | Test Loss: 9.5917\n",
      "Epoch 033 | Train Loss: 10.9270 | Test Loss: 9.4205\n",
      "Epoch 034 | Train Loss: 10.9134 | Test Loss: 9.7811\n",
      "Epoch 035 | Train Loss: 10.9200 | Test Loss: 9.8311\n",
      "Epoch 036 | Train Loss: 10.9127 | Test Loss: 9.4879\n",
      "Epoch 037 | Train Loss: 10.7685 | Test Loss: 9.6369\n",
      "Epoch 038 | Train Loss: 10.8486 | Test Loss: 9.6867\n",
      "Epoch 039 | Train Loss: 10.8554 | Test Loss: 9.6200\n",
      "Epoch 040 | Train Loss: 10.4918 | Test Loss: 9.0570\n",
      "Epoch 041 | Train Loss: 10.3626 | Test Loss: 8.9666\n",
      "Epoch 042 | Train Loss: 10.2905 | Test Loss: 8.8687\n",
      "Epoch 043 | Train Loss: 10.3307 | Test Loss: 8.6972\n",
      "Epoch 044 | Train Loss: 10.2887 | Test Loss: 8.7212\n",
      "Epoch 045 | Train Loss: 10.2596 | Test Loss: 8.6909\n",
      "Epoch 046 | Train Loss: 10.2036 | Test Loss: 8.7887\n",
      "Epoch 047 | Train Loss: 10.2219 | Test Loss: 8.8852\n",
      "Epoch 048 | Train Loss: 10.2169 | Test Loss: 8.7378\n",
      "Epoch 049 | Train Loss: 10.2117 | Test Loss: 8.8237\n",
      "Epoch 050 | Train Loss: 10.1529 | Test Loss: 8.8472\n",
      "Epoch 051 | Train Loss: 10.1782 | Test Loss: 8.6569\n",
      "Epoch 052 | Train Loss: 10.1836 | Test Loss: 8.6621\n",
      "Epoch 053 | Train Loss: 10.2197 | Test Loss: 8.6765\n",
      "Epoch 054 | Train Loss: 10.0796 | Test Loss: 8.6678\n",
      "Epoch 055 | Train Loss: 10.0905 | Test Loss: 8.7048\n",
      "Epoch 056 | Train Loss: 10.1557 | Test Loss: 8.5051\n",
      "Epoch 057 | Train Loss: 10.1316 | Test Loss: 8.7541\n",
      "Epoch 058 | Train Loss: 10.0639 | Test Loss: 8.7632\n",
      "Epoch 059 | Train Loss: 10.0862 | Test Loss: 8.8556\n",
      "Epoch 060 | Train Loss: 10.1796 | Test Loss: 8.5618\n",
      "Epoch 061 | Train Loss: 10.1275 | Test Loss: 8.6098\n",
      "Epoch 062 | Train Loss: 10.1739 | Test Loss: 8.7121\n",
      "Epoch 063 | Train Loss: 10.1084 | Test Loss: 8.6307\n",
      "Epoch 064 | Train Loss: 10.0200 | Test Loss: 8.6331\n",
      "Epoch 065 | Train Loss: 10.0863 | Test Loss: 8.6226\n",
      "Epoch 066 | Train Loss: 10.0021 | Test Loss: 8.6450\n",
      "Early stopping triggered\n",
      "\n",
      "Final RMSE: 13.7599 months\n"
     ]
    }
   ],
   "source": [
    "# In this code I enhanced the age difference between mentor and mentee to predict the match length\n",
    "# The result truned out to be much worse than the previous model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------\n",
    "# 1. Data Validation & Processing\n",
    "# ----------------------\n",
    "def load_and_validate_data():\n",
    "    # Load the dataset from a CSV file\n",
    "    df = pd.read_csv('/Users/gautam/Downloads/Training.csv', low_memory=False)\n",
    "    target = 'Match Length'\n",
    "    \n",
    "    # Remove rows with missing values in critical columns\n",
    "    # This ensures we have complete data for the target and date fields\n",
    "    df = df.dropna(subset=[target, 'Match Activation Date', 'Big Birthdate', 'Little Birthdate'])\n",
    "    \n",
    "    # Filter to keep only valid matches (positive match length)\n",
    "    df = df[df[target] > 0].copy()\n",
    "    \n",
    "    # Calculate ages based on birthdate\n",
    "    # Using a fixed reference date (February 27, 2025) for all calculations\n",
    "    # This ensures consistency but will need updating over time\n",
    "    df['Big Age'] = (pd.to_datetime('2025-02-27') - pd.to_datetime(df['Big Birthdate'])).dt.days // 365\n",
    "    df['Little Age'] = (pd.to_datetime('2025-02-27') - pd.to_datetime(df['Little Birthdate'])).dt.days // 365\n",
    "    \n",
    "    # Create derived feature: age difference between mentor and mentee\n",
    "    df['Age Difference'] = df['Big Age'] - df['Little Age']\n",
    "    \n",
    "    # Apply domain-specific validation rules:\n",
    "    # - Mentors (\"Bigs\") must be adults (18+)\n",
    "    # - Mentees (\"Littles\") must be children/youth (18 or younger)\n",
    "    df = df[(df['Big Age'] >= 18) & (df['Little Age'] <= 18)]\n",
    "    \n",
    "    # Select only the features we'll use for prediction\n",
    "    features = df[[\n",
    "        'Big Age', 'Little Age', 'Age Difference',  # Numerical features\n",
    "        'Big County', 'Big Occupation', 'Program Type'  # Categorical features\n",
    "    ]]\n",
    "    \n",
    "    return df, features, target\n",
    "\n",
    "# ----------------------\n",
    "# 2. Simplified Preprocessing\n",
    "# ----------------------\n",
    "def create_preprocessor():\n",
    "    # Create a preprocessing pipeline using scikit-learn's ColumnTransformer\n",
    "    # This allows different preprocessing for numerical vs categorical features\n",
    "    return ColumnTransformer([\n",
    "        # Pipeline for numerical features:\n",
    "        ('num', Pipeline([\n",
    "            # Replace missing values with the median of each column\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            # Standardize data: (x - mean) / std\n",
    "            # This puts all numerical features on similar scale\n",
    "            ('scaler', StandardScaler())\n",
    "        ], verbose=True), ['Big Age', 'Little Age', 'Age Difference']),\n",
    "        \n",
    "        # Pipeline for categorical features:\n",
    "        ('cat', Pipeline([\n",
    "            # Replace missing categories with the most common category\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            # Convert categories to one-hot encoded binary vectors\n",
    "            # handle_unknown='ignore' prevents errors for new categories at test time\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ], verbose=True), ['Big County', 'Big Occupation', 'Program Type'])\n",
    "    ], verbose=True, sparse_threshold=0)  # sparse_threshold=0 ensures dense output for PyTorch\n",
    "\n",
    "# ----------------------\n",
    "# 3. Simplified Model Architecture\n",
    "# ----------------------\n",
    "class MatchPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define a sequential neural network with:\n",
    "        # - 2 hidden layers (64 and 32 neurons)\n",
    "        # - ReLU activation between layers\n",
    "        # - Dropout layers to prevent overfitting\n",
    "        self.net = nn.Sequential(\n",
    "            # Input layer to first hidden layer\n",
    "            nn.Linear(input_size, 64),  # Linear transformation: y = Wx + b\n",
    "            nn.ReLU(),  # Activation function: f(x) = max(0, x)\n",
    "            nn.Dropout(0.2),  # Randomly zero 20% of neurons during training\n",
    "            \n",
    "            # First hidden layer to second hidden layer\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Second hidden layer to output (single value for regression)\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass input through the network and remove extra dimensions\n",
    "        # squeeze() removes singleton dimensions (e.g., [batch_size, 1] -> [batch_size])\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "# ----------------------\n",
    "# 4. Enhanced Training Loop\n",
    "# ----------------------\n",
    "def train_model(model, train_loader, test_loader):\n",
    "    # Set up training components:\n",
    "    \n",
    "    # HuberLoss: Less sensitive to outliers than MSE\n",
    "    # Works like MSE for small errors, like MAE for large errors\n",
    "    criterion = nn.HuberLoss()\n",
    "    \n",
    "    # Adam optimizer: Adaptive learning rates for each parameter\n",
    "    # lr=0.01 is the initial learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Learning rate scheduler: Reduces learning rate when validation loss plateaus\n",
    "    # 'patience=5' means wait 5 epochs of no improvement before reducing rate\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    \n",
    "    # Variables for early stopping\n",
    "    best_loss = float('inf')  # Track best validation loss so far\n",
    "    patience_counter = 0      # Track epochs without improvement\n",
    "    \n",
    "    # Main training loop (maximum 100 epochs)\n",
    "    for epoch in range(100):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()  # Set model to training mode (enables dropout)\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process mini-batches from training data\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Zero the parameter gradients (required each iteration)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Calculate loss between predictions and true values\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            # Maximum gradient norm of 1.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights based on gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate total loss for this epoch\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "        \n",
    "        # Compute validation loss without updating gradients\n",
    "        with torch.no_grad():\n",
    "            # Get predictions on entire test set at once\n",
    "            # (More efficient than batch-by-batch for small datasets)\n",
    "            test_preds = model(test_loader.dataset.X)\n",
    "            test_loss = criterion(test_preds, test_loader.dataset.y)\n",
    "        \n",
    "        # Adjust learning rate based on validation performance\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # --- EARLY STOPPING LOGIC ---\n",
    "        if test_loss < best_loss:\n",
    "            # If we found a better model, save it\n",
    "            best_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            # If no improvement, increment patience counter\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Print progress update\n",
    "        print(f'Epoch {epoch+1:03d} | Train Loss: {total_loss/len(train_loader):.4f} | Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        # Stop training if no improvement for 10 consecutive epochs\n",
    "        if patience_counter >= 10:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "    # Load the best model from disk\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# ----------------------\n",
    "# 5. Execution Flow\n",
    "# ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- DATA PREPARATION ---\n",
    "    # Load and clean the data\n",
    "    df, features, target = load_and_validate_data()\n",
    "    \n",
    "    # Split data into training (80%) and test (20%) sets\n",
    "    # Using random sampling with a fixed seed for reproducibility\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    test_df = df.drop(train_df.index)  # Use remaining samples for testing\n",
    "    \n",
    "    # --- FEATURE PREPROCESSING ---\n",
    "    # Create and apply the preprocessing pipeline\n",
    "    preprocessor = create_preprocessor()\n",
    "    \n",
    "    # Fit preprocessor on training data, then transform both sets\n",
    "    X_train = preprocessor.fit_transform(train_df[features.columns])\n",
    "    X_test = preprocessor.transform(test_df[features.columns])  # Only transform test data\n",
    "    \n",
    "    # --- DATASET CREATION ---\n",
    "    # Custom dataset class to handle PyTorch data loading\n",
    "    class MatchDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            # Convert numpy arrays to PyTorch tensors\n",
    "            self.X = torch.tensor(X, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y, dtype=torch.float32)\n",
    "            \n",
    "        def __len__(self):\n",
    "            # Return the number of samples\n",
    "            return len(self.X)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Return a specific sample and its label\n",
    "            return self.X[idx], self.y[idx]\n",
    "    \n",
    "    # Create dataset objects for training and testing\n",
    "    train_dataset = MatchDataset(X_train, train_df[target].values)\n",
    "    test_dataset = MatchDataset(X_test, test_df[target].values)\n",
    "    \n",
    "    # Create data loaders that will handle batching\n",
    "    # Batch size of 32 is a common choice\n",
    "    # Shuffling training data helps prevent overfitting\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # --- MODEL TRAINING ---\n",
    "    # Initialize the model with the correct input size\n",
    "    # (input_size will depend on the number of features after preprocessing)\n",
    "    model = MatchPredictor(X_train.shape[1])\n",
    "    \n",
    "    # Train the model and get back the best version\n",
    "    model = train_model(model, train_loader, test_loader)\n",
    "    \n",
    "    # --- FINAL EVALUATION ---\n",
    "    # Set model to evaluation mode for final testing\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate on the test set without updating gradients\n",
    "    with torch.no_grad():\n",
    "        # Get predictions on the test set\n",
    "        test_preds = model(test_dataset.X)\n",
    "        \n",
    "        # Calculate final RMSE (Root Mean Squared Error)\n",
    "        # Using MSE loss and taking square root\n",
    "        # This gives error in the same units as the target (months)\n",
    "        rmse = torch.sqrt(nn.MSELoss()(test_preds, test_dataset.y))\n",
    "        print(f'\\nFinal RMSE: {rmse.item():.4f} months')\n",
    "        \n",
    "        # Note: RMSE will be higher than the HuberLoss reported during training\n",
    "        # because HuberLoss reduces the impact of outliers, while RMSE does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
