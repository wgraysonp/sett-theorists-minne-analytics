{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 68.7488 | Test Loss: 50.1660\n",
      "Epoch 002 | Train Loss: 58.7098 | Test Loss: 39.5681\n",
      "Epoch 003 | Train Loss: 53.7264 | Test Loss: 39.5211\n",
      "Epoch 004 | Train Loss: 53.7060 | Test Loss: 39.4651\n",
      "Epoch 005 | Train Loss: 53.6207 | Test Loss: 39.3960\n",
      "Epoch 006 | Train Loss: 53.5934 | Test Loss: 39.3087\n",
      "Epoch 007 | Train Loss: 53.5164 | Test Loss: 39.1951\n",
      "Epoch 008 | Train Loss: 53.3234 | Test Loss: 39.0420\n",
      "Epoch 009 | Train Loss: 53.2184 | Test Loss: 38.8272\n",
      "Epoch 010 | Train Loss: 52.9417 | Test Loss: 38.5077\n",
      "Epoch 011 | Train Loss: 52.5855 | Test Loss: 37.9989\n",
      "Epoch 012 | Train Loss: 52.0614 | Test Loss: 37.1099\n",
      "Epoch 013 | Train Loss: 50.6841 | Test Loss: 35.3503\n",
      "Epoch 014 | Train Loss: 48.1798 | Test Loss: 31.5579\n",
      "Epoch 015 | Train Loss: 43.3979 | Test Loss: 25.0727\n",
      "Epoch 016 | Train Loss: 39.2301 | Test Loss: 22.5335\n",
      "Epoch 017 | Train Loss: 36.5017 | Test Loss: 21.5331\n",
      "Epoch 018 | Train Loss: 34.2303 | Test Loss: 23.6422\n",
      "Epoch 019 | Train Loss: 33.1441 | Test Loss: 25.7654\n",
      "Epoch 020 | Train Loss: 32.4256 | Test Loss: 27.0542\n",
      "Epoch 021 | Train Loss: 32.2295 | Test Loss: 27.7660\n",
      "Epoch 022 | Train Loss: 32.0270 | Test Loss: 28.0981\n",
      "Epoch 023 | Train Loss: 31.9852 | Test Loss: 28.3980\n",
      "Epoch 024 | Train Loss: 31.9126 | Test Loss: 28.6523\n",
      "Epoch 025 | Train Loss: 31.8706 | Test Loss: 28.8407\n",
      "Epoch 026 | Train Loss: 31.8269 | Test Loss: 28.9435\n",
      "Epoch 027 | Train Loss: 31.7883 | Test Loss: 28.9976\n",
      "Epoch 028 | Train Loss: 31.7960 | Test Loss: 29.1184\n",
      "Epoch 029 | Train Loss: 31.7726 | Test Loss: 29.2463\n",
      "Epoch 030 | Train Loss: 31.7830 | Test Loss: 29.2694\n",
      "Epoch 031 | Train Loss: 31.7367 | Test Loss: 29.4141\n",
      "Epoch 032 | Train Loss: 31.7240 | Test Loss: 29.4169\n",
      "Epoch 033 | Train Loss: 31.7267 | Test Loss: 29.4471\n",
      "Epoch 034 | Train Loss: 31.7312 | Test Loss: 29.5376\n",
      "Epoch 035 | Train Loss: 31.7397 | Test Loss: 29.5411\n",
      "Epoch 036 | Train Loss: 31.5941 | Test Loss: 29.5505\n",
      "Epoch 037 | Train Loss: 31.7461 | Test Loss: 29.6084\n",
      "Early stopping triggered after 20 epochs without improvement\n",
      "\n",
      "Final Test RMSE: 20.6088 months\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------\n",
    "# 1. Enhanced Interest Similarity Calculation\n",
    "# ----------------------\n",
    "def calculate_interest_similarity(df):\n",
    "    \"\"\"Improved similarity calculation with NaN handling\"\"\"\n",
    "    # This dictionary maps each interest category to a pair of column names\n",
    "    # representing interests for 'Big Contact' (mentor) and 'Little Contact' (mentee)\n",
    "    interest_pairs = {\n",
    "        'Sports': ('Big Contact: Interest Finder - Sports', \n",
    "                  'Little Contact: Interest Finder - Sports'),\n",
    "        'Hobbies': ('Big Contact: Interest Finder - Hobbies',\n",
    "                   'Little Contact: Interest Finder - Hobbies'),\n",
    "        'Entertainment': ('Big Contact: Interest Finder - Entertainment',\n",
    "                         'Little Contact: Interest Finder - Entertainment'),\n",
    "        'Places': ('Big Contact: Interest Finder - Places To Go',\n",
    "                  'Little Contact: Interest Finder - Places To Go')\n",
    "    }\n",
    "    \n",
    "    for category, (big_col, little_col) in interest_pairs.items():\n",
    "        # Handle missing values and empty strings to avoid issues in similarity calculation\n",
    "        df[big_col] = df[big_col].fillna('').astype(str)\n",
    "        df[little_col] = df[little_col].fillna('').astype(str)\n",
    "        \n",
    "        # Calculate Jaccard similarity: ratio of intersection to union of interests\n",
    "        # This measures how similar the interests are between mentors and mentees\n",
    "        # Value ranges from 0 (no overlap) to 1 (identical interests)\n",
    "        df[f'{category}_Similarity'] = df.apply(\n",
    "            lambda row: (\n",
    "                len(set(row[big_col].split(';')) & set(row[little_col].split(';'))) \n",
    "                / max(len(set(row[big_col].split(';')) | set(row[little_col].split(';'))), 1)\n",
    "            ), \n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# ----------------------\n",
    "# 2. Data Processing with Validation\n",
    "# ----------------------\n",
    "def load_and_preprocess():\n",
    "    # Load the dataset from CSV\n",
    "    df = pd.read_csv('/Users/gautam/Downloads/Training.csv', low_memory=False)\n",
    "    target = 'Match Length'\n",
    "    \n",
    "    # Filter out invalid target values - ensure all target values are present and positive\n",
    "    # This is important as we can't predict missing or invalid lengths\n",
    "    df = df[(df[target].notna()) & (df[target] > 0)].copy()\n",
    "    \n",
    "    # Calculate interest similarities between mentors and mentees\n",
    "    df = calculate_interest_similarity(df)\n",
    "    \n",
    "    # Select the features we'll use for prediction\n",
    "    # These include similarity metrics and categorical variables about participants\n",
    "    features = df[[\n",
    "        'Sports_Similarity', 'Hobbies_Similarity',\n",
    "        'Entertainment_Similarity', 'Places_Similarity',\n",
    "        'Program Type', 'Big County', 'Big Occupation',\n",
    "        'Big Languages', 'Little Contact: Language(s) Spoken'\n",
    "    ]]\n",
    "    \n",
    "    return df, features, target\n",
    "\n",
    "# ----------------------\n",
    "# 3. Advanced Preprocessing\n",
    "# ----------------------\n",
    "def create_preprocessor():\n",
    "    # Create a preprocessing pipeline with different strategies for different feature types\n",
    "    return ColumnTransformer([\n",
    "        # For similarity features: fill missing values with 0 and apply robust scaling\n",
    "        # Robust scaling is less affected by outliers than standard scaling\n",
    "        ('similarities', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), ['Sports_Similarity', 'Hobbies_Similarity',\n",
    "             'Entertainment_Similarity', 'Places_Similarity']),\n",
    "        \n",
    "        # For categorical features: fill missing values with most frequent value and apply one-hot encoding\n",
    "        # Limited to 20 most frequent categories to avoid dimensionality explosion\n",
    "        ('categories', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False,\n",
    "                                     max_categories=20))\n",
    "        ]), ['Program Type', 'Big County', 'Big Occupation']),\n",
    "        \n",
    "        # For language features: fill missing values with empty string and apply one-hot encoding\n",
    "        ('languages', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), ['Big Languages', 'Little Contact: Language(s) Spoken'])\n",
    "    ], sparse_threshold=0)  # Ensure dense output, not sparse matrices\n",
    "\n",
    "# ----------------------\n",
    "# 4. Enhanced Neural Architecture\n",
    "# ----------------------\n",
    "class InterestMatchPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Feature attention layer - learns which features are most important\n",
    "        # This allows the model to focus on the most relevant predictors\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Outputs values between 0-1 as attention weights\n",
    "        )\n",
    "        \n",
    "        # Main network architecture: a deep neural network with decreasing layer sizes\n",
    "        # Includes multiple regularization techniques to prevent overfitting\n",
    "        self.net = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),  # Stabilizes learning by normalizing layer inputs\n",
    "            nn.ReLU(),            # Non-linear activation\n",
    "            nn.Dropout(0.4),      # Randomly drops 40% of neurons during training to prevent overfitting\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),      # Slightly less dropout in deeper layers\n",
    "            \n",
    "            # Third hidden layer\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),     # Layer normalization as an alternative to batch norm\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer - single neuron for regression output\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply attention mechanism: multiply each feature by its learned importance weight\n",
    "        attn_weights = self.attention(x)\n",
    "        x = x * attn_weights  # Element-wise multiplication for feature-wise attention\n",
    "        \n",
    "        # Pass weighted features through the main network\n",
    "        return self.net(x).squeeze()  # Squeeze removes singleton dimension from output\n",
    "\n",
    "# ----------------------\n",
    "# 5. Optimized Training Loop\n",
    "# ----------------------\n",
    "def train_model(model, train_loader, test_loader):\n",
    "    # Huber loss is more tolerant to outliers than MSE - good for duration predictions\n",
    "    # which may have some extreme values\n",
    "    criterion = nn.HuberLoss(delta=2.0)  \n",
    "    \n",
    "    # AdamW optimizer: Adam with decoupled weight decay for better regularization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate scheduler: reduces learning rate according to cosine schedule with restarts\n",
    "    # Helps escape local minima and converge to better solutions\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 20  # Stop if no improvement for 20 epochs\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(100):  # Maximum of 100 epochs\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Standard training step\n",
    "            optimizer.zero_grad()      # Clear previous gradients\n",
    "            outputs = model(X_batch)   # Forward pass\n",
    "            loss = criterion(outputs, y_batch)  # Calculate loss\n",
    "            loss.backward()            # Backward pass\n",
    "            \n",
    "            # Gradient clipping prevents exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            \n",
    "            optimizer.step()           # Update weights\n",
    "            total_loss += loss.item()  # Accumulate loss for reporting\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "        with torch.no_grad():  # No need to track gradients during validation\n",
    "            test_preds = model(test_loader.dataset.X)\n",
    "            test_loss = criterion(test_preds, test_loader.dataset.y)\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Calculate and report average training loss\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1:03d} | Train Loss: {avg_loss:.4f} | Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            patience = 0\n",
    "            # Save the best model so far\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        if patience >= max_patience:\n",
    "            print(f\"Early stopping triggered after {max_patience} epochs without improvement\")\n",
    "            break\n",
    "            \n",
    "    # Load the best model state before returning\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# ----------------------\n",
    "# 6. Execution Flow with Monitoring\n",
    "# ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and process data\n",
    "    df, features, target = load_and_preprocess()\n",
    "    \n",
    "    # Temporal split with validation - sort by date for more realistic evaluation\n",
    "    # This is better than random splitting for time-dependent data\n",
    "    df = df.sort_values('Match Activation Date')\n",
    "    \n",
    "    # Split into train (70%), validation (15%), and test (15%) sets\n",
    "    train_size = int(len(df) * 0.7)\n",
    "    val_size = int(len(df) * 0.15)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    val_df = df.iloc[train_size:train_size+val_size]\n",
    "    test_df = df.iloc[train_size+val_size:]\n",
    "    \n",
    "    # Apply preprocessing pipeline to each dataset split\n",
    "    preprocessor = create_preprocessor()\n",
    "    X_train = preprocessor.fit_transform(train_df[features.columns])  # Fit on training set only\n",
    "    X_val = preprocessor.transform(val_df[features.columns])          # Apply to validation set\n",
    "    X_test = preprocessor.transform(test_df[features.columns])        # Apply to test set\n",
    "    \n",
    "    # Create PyTorch datasets for efficient batching and iteration\n",
    "    class MatchDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            # Convert numpy arrays to PyTorch tensors\n",
    "            self.X = torch.tensor(X, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y, dtype=torch.float32)\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "    \n",
    "    # Create datasets for each split\n",
    "    train_dataset = MatchDataset(X_train, train_df[target].values)\n",
    "    val_dataset = MatchDataset(X_val, val_df[target].values)\n",
    "    test_dataset = MatchDataset(X_test, test_df[target].values)\n",
    "    \n",
    "    # Create data loaders for batched processing\n",
    "    batch_size = 128  # Process 128 samples at a time\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle training data\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model with appropriate input size\n",
    "    model = InterestMatchPredictor(X_train.shape[1])\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(test_dataset.X)\n",
    "        # Calculate RMSE: root mean squared error in months\n",
    "        rmse = torch.sqrt(nn.MSELoss()(test_preds, test_dataset.y))\n",
    "        print(f'\\nFinal Test RMSE: {rmse.item():.4f} months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
